
function train(algorithm::DDPG, l::Learner)

    scores = zeros(100)
    e = 1
    idx = 1

    while e <= p.max_episodes

        ep = Episode(env, l, p)()


        for (s, a, r, sâ€², t) in ep.episode

            remember(p.mem_size, s, a, r, sâ€², t)
            p.frames += 1

            # if length(ð’Ÿ) >= p.train_start# && Ï€.train
            if p.frames >= p.train_start# && Ï€.train

                S, A, R, Sâ€² = sampleBuffer(l.serial)

                Aâ€² = Î¼Ï•â€²(Sâ€²)
                Vâ€² = QÎ¸â€²(vcat(Sâ€², Aâ€²))
                Y = R + p.Î³ * Vâ€²

                # critic
                dÎ¸ = gradient(() -> loss(Critic(), Y, S, A), Flux.params(QÎ¸))
                update!(Optimise.Adam(p.Î·_critic), Flux.params(QÎ¸), dÎ¸)

                # actor
                dÏ• = gradient(() -> -loss(Actor(), S), Flux.params(Î¼Ï•))
                update!(Optimise.Adam(p.Î·_actor), Flux.params(Î¼Ï•), dÏ•)


                for (base, target) in zip(Flux.params(QÎ¸), Flux.params(QÎ¸â€²))
                    target .= p.Ï„_critic * base .+ (1 - p.Ï„_critic) * target
                end

                for (base, target) in zip(Flux.params(Î¼Ï•), Flux.params(Î¼Ï•â€²))
                    target .= p.Ï„_actor * base .+ (1 - p.Ï„_actor) * target
                end
            end
            
        end
        

        scores[idx] = ep.total_reward
        idx = idx % 100 + 1
        avg = mean(scores)
        if (e-1) % 10 == 0
            println("Episode: $e | Score: $(ep.total_reward) | Avg score: $avg | Frames: $(p.frames)")
        end
        e += 1

        append!(p.total_rewards, ep.total_reward)

    end

end






function train(algorithm::NODEModel, l::Learner)

    # scores = zeros(100)
    # e = 1
    # idx = 1

    for j in 1:p.Sequences

        ep = Episode(env, l, p)()


        for (s, a, r, sâ€², t) in ep.episode

            remember(p.mem_size, s, a, r, sâ€², t)

        end
        

        S, A, R, Sâ€² = sampleBuffer(l.serial)


        for i in 1:p.batch_size
        
            dÎ¸ = gradient(() -> loss(NODE(), S[:,:,i], A[:,:,i], R[:,:,i], Sâ€²[:,:,i]), params(fÎ¸))
            update!(Optimise.Adam(p.Ï„_actor), Flux.params(fÎ¸), dÎ¸)

            dÏ• = gradient(() -> loss(Reward(), S[:,:,i], A[:,:,i], R[:,:,i], Sâ€²[:,:,i]), params(RÏ•))
            update!(Optimise.Adam(p.Ï„_critic), Flux.params(RÏ•), dÏ•)

            append!(p.model_loss, loss(NODE(), S[:,:,i], A[:,:,i], R[:,:,i], Sâ€²[:,:,i]))
            append!(p.reward_loss, loss(Reward(), S[:,:,i], A[:,:,i], R[:,:,i], Sâ€²[:,:,i]))
        
        end


        if j % 10 == 0
            println("Iteration $j || Model loss $(p.model_loss[end]) || Reward loss $(p.reward_loss[end])")
        end


    end

end



function train(algorithm::DynaWorldModel, l::Learner)


    for j in 1:p.Sequences

        ep = Episode(env, l, p)()


        for (s, a, r, sâ€², t) in ep.episode

            remember(p.mem_size, s, a, r, sâ€², t)

        end
        

        S, A, R, Sâ€² = sampleBuffer(l.serial)


        for i in 1:p.batch_size
        
            dÎ¸ = gradient(() -> loss(DyNODE(), S[:,:,i], A[:,:,i], R[:,:,i], Sâ€²[:,:,i]), params(fÎ¸))
            update!(Optimise.Adam(0.005), Flux.params(fÎ¸), dÎ¸)

            dÏ• = gradient(() -> loss(DyReward(), S[:,:,i], A[:,:,i], R[:,:,i], Sâ€²[:,:,i]), params(RÏ•))
            update!(Optimise.Adam(0.005), Flux.params(RÏ•), dÏ•)

            append!(p.model_loss, loss(DyNODE(), S[:,:,i], A[:,:,i], R[:,:,i], Sâ€²[:,:,i]))
            append!(p.reward_loss, loss(DyReward(), S[:,:,i], A[:,:,i], R[:,:,i], Sâ€²[:,:,i]))
        
        end


        if j % 10 == 0
            println("Iteration $j || Model loss $(p.model_loss[end]) || Reward loss $(p.reward_loss[end])")
        end


    end

end






